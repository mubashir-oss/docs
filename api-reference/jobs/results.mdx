---
title: "Get Extraction Result"
openapi: "/api-reference/openapi.json GET /extract/{job_id}"
description: "Retrieve the results of a completed processing job"
---

## Overview

The Get Job Results endpoint retrieves the processed data from a completed job. This endpoint should only be called after confirming the job status is "COMPLETED" using the status endpoint.

<Note>
Results are only available for completed jobs. Check job status first to ensure processing has finished.
</Note>

## Response

The response structure depends on the job type (extraction, parsing, classification, etc.).

### Extraction Job Results

<ResponseField name="job_id" type="string">
  Unique identifier for the extraction job
</ResponseField>

<ResponseField name="status" type="string">
  Current status of the job (e.g., "completed", "processing", "failed")
</ResponseField>

<ResponseField name="file_name" type="string | null">
  Original filename of the uploaded document
</ResponseField>

<ResponseField name="created_at" type="string">
  ISO 8601 timestamp when the job was created
</ResponseField>

<ResponseField name="updated_at" type="string">
  ISO 8601 timestamp when the job was last updated
</ResponseField>

<ResponseField name="metadata" type="object">
  Job metadata containing:
  - `order`: Array of extracted field names in order
  - `page_count`: Number of pages in the document
</ResponseField>

<ResponseField name="result" type="object">
  The extracted data matching the provided JSON schema. Each field contains:
  - `score`: Confidence score (0-1) for the extraction
  - `value`: The extracted value
  - `bboxes`: Array of bounding box objects with `bbox` coordinates
  - `page_no`: Page number where the value was found (1-indexed)
</ResponseField>

<RequestExample>

```bash cURL
curl -X GET "https://prod.visionapi.unsiloed.ai/extract/{job_id}" \
  -H "api-key: your-api-key"
```

```python Python
import requests

job_id = "b2094b38-e432-44b6-a5d0-67bed07d5de1"
url = f"https://prod.visionapi.unsiloed.ai/extract/{job_id}"
headers = {"api-key": "your-api-key"}

response = requests.get(url, headers=headers)

if response.status_code == 200:
  response = response.json()
    
  print(json.dumps(response, indent=4))
```

```javascript JavaScript
const jobId = 'b2094b38-e432-44b6-a5d0-67bed07d5de1';
const response = await fetch(`https://prod.visionapi.unsiloed.ai/extract/${jobId}`, {
  headers: {
    'api-key': 'your-api-key'
  }
});


if (response.ok) {
  const result = await response.json();
  console.log(result);
} else {
  console.error(
    `Request failed with status ${response.status}: ${response.statusText}`
  );
}
```

</RequestExample>

<ResponseExample>

```json Extraction Results
{
  "job_id": "36b2c5dc-942c-4b20-8451-39764246f9aa",
  "status": "completed",
  "file_name": "invoice.pdf",
  "created_at": "2026-01-19T21:10:51.324693+00:00",
  "updated_at": "2026-01-19T21:10:55.165642+00:00",
  "metadata": {
    "order": ["invoice_number", "company_name", "total_amount"],
    "page_count": 1
  },
  "result": {
    "invoice_number": {
      "score": 0.7970692802977641,
      "value": "25G1IZT000009999",
      "bboxes": [
        {
          "bbox": [150, 80, 320, 110]
        }
      ],
      "page_no": 1
    },
    "company_name": {
      "score": 0.9945,
      "value": "Acme Corporation",
      "bboxes": [
        {
          "bbox": [72, 100, 400, 140]
        }
      ],
      "page_no": 1
    },
    "total_amount": {
      "score": 0.9876,
      "value": "1250000",
      "bboxes": [
        {
          "bbox": [150, 250, 300, 280]
        }
      ],
      "page_no": 1
    }
  }
}
```

```json Processing
{
  "job_id": "36b2c5dc-942c-4b20-8451-39764246f9aa",
  "status": "queued",
  "message": "PDF citation processing started",
  "quota_remaining": 91242
}
```

</ResponseExample>

## Complete Workflow Example

Here's a complete example of submitting a job, monitoring its progress, and retrieving results:

```python
import requests
import time
import json

def process_document_with_results(file_path, schema, api_key):
    """Complete workflow: submit job, wait for completion, get results"""
    
    headers = {"api-key": api_key}
    
    # Step 1: Submit extraction job
    files = {"pdf_file": open(file_path, "rb")}
    data = {"schema_data": json.dumps(schema)}
    
    response = requests.post(
        "https://prod.visionapi.unsiloed.ai/extract",
        files=files,
        data=data,
        headers=headers
    )
    
    if response.status_code != 200:
        raise Exception(f"Failed to submit job: {response.text}")
    
    job_id = response.json()["job_id"]
    print(f"Job submitted: {job_id}")
    
    # Step 2: Poll for completion
    while True:
        status_response = requests.get(
            f"https://prod.visionapi.unsiloed.ai/extract/{job_id}",
            headers=headers
        )
        
        if status_response.status_code != 200:
            raise Exception(f"Failed to check status: {status_response.text}")
            
        job = status_response.json()
        status = job["status"]
        print(f"Job status: {status}")
        
        if status == "COMPLETED" or status == "completed":
            break
        elif status == "FAILED" or status == "failed":
            raise Exception(f"Job failed: {job.get('error', 'Unknown error')}")
        
        time.sleep(5)  # Wait 5 seconds before checking again
    
    # Step 3: Get results
    results_response = requests.get(
        f"https://prod.visionapi.unsiloed.ai/extract/{job_id}",
        headers=headers
    )
    
    if results_response.status_code != 200:
        raise Exception(f"Failed to get results: {results_response.text}")
    
    return results_response.json()

# Usage example
schema = {
    "type": "object",
    "properties": {
        "company_name": {
            "type": "string",
            "description": "Name of the company"
        },
        "total_amount": {
            "type": "number",
            "description": "Total financial amount"
        }
    },
    "required": ["company_name"],
    "additionalProperties": False
}

try:
    results = process_document_with_results("document.pdf", schema, "your-api-key")
    print("Extraction completed!")
    print("Results:", results)
except Exception as e:
    print(f"Error: {e}")
```

## Result Data Structure

### Extracted Field Format

Each extracted field in the `result` object contains:
- `score`: A confidence score between 0 and 1 indicating extraction confidence
- `value`: The extracted value as a string
- `bboxes`: Array of bounding box objects indicating where the value was found
- `page_no`: The page number where the value was located (1-indexed)

### Bounding Box Format

Bounding boxes use the format `[x1, y1, x2, y2]` where:
- `x1, y1`: Top-left corner coordinates
- `x2, y2`: Bottom-right corner coordinates
- Coordinates are in pixels from the top-left of the page
- Page numbers are 1-indexed

### Confidence Scores

- **0.9-1.0**: Very high confidence, extraction is very likely correct
- **0.8-0.9**: High confidence, extraction is likely correct
- **0.7-0.8**: Good confidence, may warrant review for critical applications
- **0.6-0.7**: Medium confidence, should be reviewed
- **Below 0.6**: Low confidence, likely needs manual verification

## Error Handling

<AccordionGroup>
  <Accordion title="Job Still Processing (400)">
    The job hasn't completed yet. Wait for the status to change to "COMPLETED" before requesting results.
  </Accordion>

  <Accordion title="Job Failed (500)">
    The job encountered an error during processing. Check the error message for details about what went wrong.
  </Accordion>

  <Accordion title="Results Not Found (404)">
    Either the job doesn't exist, or the results have been cleaned up. Results are typically stored for 7 days.
  </Accordion>

  <Accordion title="Job Not Found (404)">
    The job ID is invalid or the job has been deleted. Verify you're using the correct job ID.
  </Accordion>
</AccordionGroup>
