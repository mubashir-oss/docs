---
title: "Document Parsing"
description: "Extract structured content from documents using the Unsiloed Python SDK"
---

# Document Parsing

The Parse API extracts structured content from documents (PDFs, images, etc.) with advanced layout detection, table extraction, and OCR capabilities.

## Basic Usage

### Synchronous

```python
from unsiloed_sdk import UnsiloedClient

with UnsiloedClient(api_key="your-api-key") as client:
    result = client.parse_and_wait(
        file="document.pdf",
        merge_tables=True
    )

    print(f"Status: {result.status}")
    print(f"Total chunks: {result.total_chunks}")

    for chunk in result.chunks:
        print(f"Page {chunk['page_number']}: {len(chunk['segments'])} segments")
```

### Async

```python
import asyncio
from unsiloed_sdk import AsyncUnsiloedClient

async def main():
    async with AsyncUnsiloedClient(api_key="your-api-key") as client:
        result = await client.parse_and_wait(
            file="document.pdf",
            merge_tables=True
        )

        for chunk in result.chunks:
            for segment in chunk['segments']:
                print(f"{segment['segment_type']}: {segment.get('content', '')[:100]}")

asyncio.run(main())
```

## Parse Methods

The SDK provides three methods for parsing:

### 1. parse_and_wait() - Automatic Polling (Recommended)

The simplest approach - submits the document and automatically waits for completion:

```python
result = client.parse_and_wait(
    file="document.pdf",
    merge_tables=True,
    poll_interval=2.0,  # Check status every 2 seconds
    max_wait=600.0      # Maximum wait time in seconds
)
```

### 2. parse() - Start Job

Submits a document for parsing and returns immediately:

```python
response = client.parse(
    file="document.pdf",
    merge_tables=True
)

print(f"Job ID: {response.job_id}")
print(f"Status: {response.status}")
```

### 3. get_parse_result() - Check Status

Check the status of a parsing job:

```python
result = client.get_parse_result(job_id="your-job-id")

if result.status in ["Succeeded", "completed"]:
    print(f"Parsing complete! Total chunks: {result.total_chunks}")
elif result.status in ["Failed", "failed"]:
    print(f"Parsing failed: {result.error}")
else:
    print(f"Still processing... Status: {result.status}")
```

## Input Options

### Parse from File Path

```python
result = client.parse_and_wait(file="path/to/document.pdf")
```

### Parse from File Bytes

```python
with open("document.pdf", "rb") as f:
    file_bytes = f.read()

result = client.parse_and_wait(file=file_bytes)
```

### Parse from URL

```python
result = client.parse_and_wait(
    url="https://example.com/document.pdf"
)
```

<Note>
  You must provide either `file` or `url`, but not both.
</Note>

## Parse Parameters

### Basic Parameters

```python
result = client.parse_and_wait(
    file="document.pdf",
    merge_tables=True,              # Merge consecutive table segments
    enhanced_table=False,           # Use LLM for enhanced table processing
    validate_table_segments=False,  # Validate tables using VLM
    use_high_resolution=False       # Enable high-resolution processing
)
```

### Advanced Parameters

```python
result = client.parse_and_wait(
    file="document.pdf",

    # Segmentation
    segmentation_method="smart_layout_detection",  # or "page"

    # OCR Settings
    ocr_mode="auto_ocr",           # "auto_ocr", "force_ocr", or "no_ocr"
    ocr_engine="UnsiloedHawk",     # OCR engine to use

    # Filtering
    keep_segment_types="Text,Table,Picture",  # Comma-separated list or "all"

    # Error Handling
    error_handling="Continue",     # How to handle errors

    # Segment Analysis
    segment_analysis={
        "Table": {
            "html": "LLM",
            "markdown": "LLM",
            "extended_context": True,
            "crop_image": "All"
        },
        "Picture": {
            "crop_image": "All",
            "html": "LLM",
            "markdown": "LLM"
        }
    }
)
```

### Parameter Reference

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `file` | `str \| Path \| bytes` | - | File path or bytes to parse |
| `url` | `str` | - | URL of document to parse |
| `merge_tables` | `bool` | `False` | Merge consecutive table segments |
| `enhanced_table` | `bool` | `False` | Use LLM for table processing |
| `validate_table_segments` | `bool` | `False` | Validate tables with VLM |
| `use_high_resolution` | `bool` | `False` | Enable high-res processing |
| `segmentation_method` | `str` | `"smart_layout_detection"` | Layout detection method |
| `ocr_mode` | `str` | `"auto_ocr"` | OCR strategy |
| `ocr_engine` | `str` | `"UnsiloedHawk"` | OCR engine |
| `keep_segment_types` | `str` | `"all"` | Segment types to keep |
| `error_handling` | `str` | `"Continue"` | Error handling strategy |

## Response Structure

### ParseResponse Object

```python
@dataclass
class ParseResponse:
    job_id: str                           # Unique job identifier
    status: str                           # Job status
    file_name: Optional[str]              # Original file name
    created_at: Optional[str]             # Job creation time
    started_at: Optional[str]             # Processing start time
    finished_at: Optional[str]            # Processing finish time
    message: Optional[str]                # Status message
    credit_used: Optional[int]            # Credits consumed
    quota_remaining: Optional[int]        # Remaining quota
    total_chunks: Optional[int]           # Number of chunks
    chunks: Optional[List[Dict]]          # Parsed content chunks
    error: Optional[str]                  # Error message if failed
```

### Chunk Structure

Each chunk represents a page or section of the document:

```python
{
    "page_number": 1,
    "segments": [
        {
            "segment_type": "Text",
            "content": "Document content here...",
            "bbox": [x1, y1, x2, y2],
            "confidence": 0.98
        },
        {
            "segment_type": "Table",
            "content": "...",
            "html": "<table>...</table>",
            "markdown": "| Col1 | Col2 |\n|------|------|",
            "bbox": [x1, y1, x2, y2]
        }
    ]
}
```

## Advanced Examples

### Parsing with Table Enhancement

Extract tables with enhanced formatting:

```python
result = client.parse_and_wait(
    file="report.pdf",
    merge_tables=True,
    enhanced_table=True,
    segment_analysis={
        "Table": {
            "html": "LLM",
            "markdown": "LLM",
            "extended_context": True
        }
    }
)

# Process tables
for chunk in result.chunks:
    for segment in chunk['segments']:
        if segment['segment_type'] == 'Table':
            print(f"Table found on page {chunk['page_number']}")
            print(segment['markdown'])
```

### Filtering Segment Types

Only extract specific content types:

```python
result = client.parse_and_wait(
    file="document.pdf",
    keep_segment_types="Text,Table"  # Only keep text and tables
)

# Count segment types
from collections import Counter
segment_types = []
for chunk in result.chunks:
    for segment in chunk['segments']:
        segment_types.append(segment['segment_type'])

print(Counter(segment_types))
```

### High-Resolution Processing

For documents with small text or complex layouts:

```python
result = client.parse_and_wait(
    file="technical_diagram.pdf",
    use_high_resolution=True,
    ocr_mode="force_ocr",
    validate_table_segments=True
)
```

### Manual Polling with Custom Logic

For more control over the polling process:

```python
import asyncio
from unsiloed_sdk import AsyncUnsiloedClient

async def parse_with_custom_polling():
    async with AsyncUnsiloedClient(api_key="your-api-key") as client:
        # Start the parse job
        response = await client.parse(file="large_document.pdf")
        print(f"Job started: {response.job_id}")

        # Custom polling logic
        max_attempts = 100
        attempt = 0

        while attempt < max_attempts:
            result = await client.get_parse_result(job_id=response.job_id)

            print(f"Attempt {attempt + 1}: Status = {result.status}")

            if result.status in ["Succeeded", "completed"]:
                print(f"Success! Total chunks: {result.total_chunks}")
                return result
            elif result.status in ["Failed", "failed"]:
                print(f"Failed: {result.error}")
                return result

            # Exponential backoff
            wait_time = min(2 ** attempt, 30)
            await asyncio.sleep(wait_time)
            attempt += 1

        print("Timeout: Job did not complete")
        return None

asyncio.run(parse_with_custom_polling())
```

### Concurrent Document Parsing

Process multiple documents simultaneously:

```python
import asyncio
from unsiloed_sdk import AsyncUnsiloedClient

async def parse_multiple_documents():
    async with AsyncUnsiloedClient(api_key="your-api-key") as client:
        files = ["doc1.pdf", "doc2.pdf", "doc3.pdf", "doc4.pdf"]

        # Parse all documents concurrently
        tasks = [client.parse_and_wait(file=f) for f in files]
        results = await asyncio.gather(*tasks)

        for file, result in zip(files, results):
            print(f"{file}: {result.status} - {result.total_chunks} chunks")

asyncio.run(parse_multiple_documents())
```

## Processing Results

### Extract All Text

```python
def extract_text(result):
    all_text = []
    for chunk in result.chunks:
        for segment in chunk['segments']:
            if segment['segment_type'] == 'Text':
                all_text.append(segment.get('content', ''))
    return '\n'.join(all_text)

result = client.parse_and_wait(file="document.pdf")
text = extract_text(result)
print(text)
```

### Extract Tables as DataFrames

```python
import pandas as pd
from io import StringIO

def extract_tables(result):
    tables = []
    for chunk in result.chunks:
        for segment in chunk['segments']:
            if segment['segment_type'] == 'Table' and 'markdown' in segment:
                # Convert markdown table to DataFrame
                df = pd.read_table(StringIO(segment['markdown']), sep='|', header=0)
                df = df.iloc[:, 1:-1]  # Remove empty first/last columns
                df.columns = df.columns.str.strip()
                tables.append({
                    'page': chunk['page_number'],
                    'dataframe': df
                })
    return tables

result = client.parse_and_wait(
    file="report.pdf",
    merge_tables=True,
    enhanced_table=True,
    keep_segment_types="Table"
)

tables = extract_tables(result)
for i, table_info in enumerate(tables):
    print(f"Table {i+1} from page {table_info['page']}:")
    print(table_info['dataframe'])
```

## Error Handling

```python
from unsiloed_sdk import (
    UnsiloedClient,
    AuthenticationError,
    InvalidRequestError,
    TimeoutError,
    APIError
)

try:
    with UnsiloedClient(api_key="your-api-key") as client:
        result = client.parse_and_wait(
            file="document.pdf",
            max_wait=300  # 5 minutes
        )

        if result.status in ["Failed", "failed"]:
            print(f"Parse failed: {result.error}")
        else:
            print(f"Success: {result.total_chunks} chunks")

except AuthenticationError:
    print("Invalid API key")
except InvalidRequestError as e:
    print(f"Invalid request: {e.message}")
except TimeoutError:
    print("Request timed out")
except APIError as e:
    print(f"API error: {e.message}")
```

## Best Practices

<Steps>
  <Step title="Use parse_and_wait() for Simplicity">
    Unless you need fine-grained control, use the automatic polling method
  </Step>

  <Step title="Enable merge_tables">
    Set `merge_tables=True` for better table extraction in most cases
  </Step>

  <Step title="Filter Unnecessary Segments">
    Use `keep_segment_types` to reduce processing time and response size
  </Step>

  <Step title="Process Large Documents Asynchronously">
    Use `AsyncUnsiloedClient` for large documents or batch processing
  </Step>

  <Step title="Handle Errors Gracefully">
    Always check `result.status` and handle potential errors
  </Step>
</Steps>

## Next Steps

<CardGroup cols={2}>
  <Card title="Extraction" icon="database" href="/sdks/python/extraction">
    Extract structured data using schemas
  </Card>
  <Card title="Classification" icon="tags" href="/sdks/python/classification">
    Classify documents into categories
  </Card>
</CardGroup>
